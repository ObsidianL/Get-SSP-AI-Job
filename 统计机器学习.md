[TOC]

Talk is cheap, show me your code.

## 概论

1. 假设：**独立同分布**的数据：

    同分布：

    我们的数据是从确定的某个系统产生的，其模型结构和参数是一样的，如果不一样就没有意义了。

    我们的建模工作，实际上是基于观测数据去推测研究对象的结构。由于我们的观测能力、计算能力有限，无法获取“全量”或者“全部”的数据，因此，获取的观测数据集一般规模有限，是对总体的一个抽样。我们希望抽样尽可能地和总体相似，也就是服从相同的分布；而抽样里的每一个样本，都是同一个系统生成的，因此都服从同一个分布。我们称系统生成的所有样本为总体，其中被我们观测到的部分为抽样。“独立同分布”的含义也可以这样表述：抽样内样本服从总体的分布。

    独立：每一个样本的出现或者生成都是独立事件【当然是假设】

    可以基于随机抽样来获取满足独立同分布假设的数据集。

2. 贝叶斯学派（贝叶斯估计）和频率学派（极大似然估计）

3. 核方法

4. 三要素：

    1. 模型：
    2. 策略：定义模型好坏程度
    3. 算法：反向传播，也就是如何寻找到最优参数

5. 模型选择：正则化（选取泛化能力强的模型）、交叉验证（也是选取泛化能力强的模型）

6. 生成模型 vs 判别模型



## 感知机

具体的感知机算法写出流程：`p38 2.3.1`

线性分类、判别模型

未看知识点：收敛性以及对偶问题

1. 模型：$f(x) = sign(wx+b)$超平面$wx+b = 0$
2. 损失函数：误分类点到超平面的距离，去掉了范数项，其求导
3. 优化方法：随机梯度下降，每次一个误分类点下降，梯度下降公式
4. 推导感知机（原始形式）





## 朴素贝叶斯

具体的朴素贝叶斯算法需要会写出流程：`p62 4.2.2`

基本概念：

1. 先验概率  后验概率 似然概率

    贝叶斯公式理解之似然函数，后验概率，先验概率 - 科研狗也有春天的文章 - 知乎 https://zhuanlan.zhihu.com/p/479174192

    ![image-20220316110845801](https://picgo-an.oss-cn-guangzhou.aliyuncs.com/img/image-20220316110845801.png)

    最大后验概率，就是选择p最大的那个$\theta$

2. 朴素贝叶斯的流程：通过数据学得联合分布，而后通过贝叶斯定理进行预测。

    $P(Y=c_k|X=x) = \dfrac{P(Y=c_k,X=x)}{P(X=x)}=\dfrac{P(X=x|Y=c_k)P(Y=c_k)}{\sum{P(X=x|Y=c_k)}}$

3. 朴素贝叶斯有一个强假设：条件独立性，也就是说，特征x之间是独立的。因此有

4. 将x输入到$P(Y=c_k|X)$中，选出让P最大的$c_k$作为类别，$y = c_k$



## 决策树——重中之重

一篇文章搞定GBDT、Xgboost和LightGBM的面试 - 丢丢的文章 - 知乎 https://zhuanlan.zhihu.com/p/148050748



决策树就是if-then规则的集合

决策树叶子节点表示某类，而非叶子节点则表示某一个属性，其出边代表着划分。流程：

1. 特征选择，信息增益排序，算法在p74

2. 决策树的生成，选择最优（信息增益最大的特征）特征将训练集进行划分，划分之后再选最优特征进行划分，直到划分的子集足够“纯粹“
3. 决策树的剪枝，防止过拟合，让树结构更加简单



特征选择要选取对训练数据具有分类能力的特征，而衡量标准就是信息增益，也就是固定某个特征取值之后，其熵的变化程度。

熵：不确定程度，比如硬币正反面概率为0.5 0.5 不确定性最大，0 vs 1则不确定性为0，衡量：$-\sum{p_ilog_2{p_i}}$

条件熵：在固定某一个特征时，即以某一特征为条件时的条件概率

信息增益：以特征A为条件时，熵-条件熵的差值，也就是当已知特征A时，我们取猜测Y的不确定性减小的程度

信息增益比：信息增益会偏向特征取值较多的特征，因此增加了一个分母，分母是训练集关于该特征的熵，也就是把该特征当做标签时的熵。



ID3：依据信息增益选取增益最大的特征，并进行分裂，如果信息增益小于某个阈值则不再分裂。分裂就是依据特征的取值，分成不同的集合。不断分裂直到停止，逐渐构成树。ID3只是树的生成，不剪枝，因此容易过拟合

C4.5：在ID3基础上选取信息增益比来作为标准



决策树剪枝：定义一个损失函数，由两部分构成，一部分是模型的误差（不同决策树采取的误差衡量不一样），一部分是模型复杂度|T|（比如叶结点个数）。递归收缩树结点，此时误差肯定变大，但是复杂度变小，如果收缩后损失函数变小，那么就收缩，否则不收缩。



### CART算法 Classfiction And Regression Tree

CART是二叉树式的决策树，分裂的时候只分左右子树，也就是依据最优特征上的最优二值切分点进行分裂。

停止分裂的条件有：MSE或者Gini小于阈值，或者集合中的样本数小于阈值。

#### CART回归树

利用平方误差来选择最优特征以及最优二值切分点p82

递归对数据集进行划分，而针对某一个数据集，也就是某个结点对于的数据集，遍历特征j，以及特征j的划分点s（特征小于s的分到左子树，特征大于等于s的分到右子树）。相当于两层循环取找最优秀的(j,s)，对于某一个(j,s)划分之后的两个集合R1（特征j小于s）和R2（特征j大于等于s），计算集合的$\sum(y_i - c_m)^2$就是方差没有分母，选择两个集合的MSE最小的(j,s)进行分裂。

输出是叶结点对应集合的平均标签。

#### CART分类树

利用基尼指数来选择最优特征以及最优二值切分点p84

基尼指数表示集合D的不确定性，$Gini(D)=1-\sum(\dfrac{|C_k|}{|D|})^2$

遍历特征A，以及其可能的取值a，按照A是否为a来划分出集合R1和R2，计算两个集合的Gini指数和，取最小的(A, a)组合，进行递归分裂

#### CART剪枝-此处不是很明白

第五章 决策树（第5节 CART算法 第2小节 CART剪枝） - Arlen的文章 - 知乎 https://zhuanlan.zhihu.com/p/76709712

剪枝过程中的损失函数$C_\alpha(T) = C(T) +\alpha|T|$，第一项是预测误差（对应MSE、Gini），第二项是子树的叶节点个数，$\alpha$用于权衡准确度和复杂度。

不断剪枝形成子树序列$T_0, T_1....T_n$，利用损失函数以及交叉验证选取最优子树



![preview](https://picgo-an.oss-cn-guangzhou.aliyuncs.com/img/v2-274ad10bbc810467c3f83fcfdd9a2871_r.jpg)



#### XGBoost



#### GDBT




#### LightGBM

